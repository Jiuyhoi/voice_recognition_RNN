{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The destination name is too long (1139), reducing to 236\n",
      "--2023-03-25 18:43:09--  https://mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com/cv-corpus-1/en.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ3GQRTO3IFDOHBJP%2F20230325%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230325T174114Z&X-Amz-Expires=43200&X-Amz-Security-Token=FwoGZXIvYXdzEDsaDBC4zPpDJq12Yv6tXSKSBONMEgB3gdLg5c3lUHnm3%2BucrTCYrctbmwFGg6KjUPbnBzCCboMxHOOcssMQkC5uoDakMhZUm4cOTiw8T5dch%2FS8thbTmObPUr0FiIpmCV65vS%2FRHui6ZIZ5NvaTmjRRk1ZtF1EHDQNeSYouN0xAcpLzjtP2XXkT7kKDPhwP55MdZr87dw4WPk9LbUyNNM%2B7Owqo58vHzBKkEEzs%2FKhkqzBI3CVXkxn8goq%2BeJI1yw8bj4mPAW0DNezP%2FV3a3qPgm%2F20uZ2qodqr3ISMey0HpsVQ4PvFvAIvfasLOGRLBqgnDVIHYZ%2B4pAJ%2FVrDFxqoYXzlINO1XHX6HVGhqmQu%2Bmwwy9J56Hyb%2B%2FLeO3mWEcjjcz7mq55I4CSwZhSkbBP8G8ki5osVn%2BIwGrSyy6d5PYz1CZjs2%2BiBavIPPvRVRvDBpHiC51sMUCUxGj31n5WpG%2B8AhfIpQgsNKq3oAqbVc8u5V7yrwCmIrAqMQ7R2dt%2FgalA7YbwpsHkKVAJjyDgZQV%2FojkNqApfCKA0K%2FiaCn4qOQmWOGTbmWuvgoM9aXJxDwqrzttRtK1zaNnGAuuKwQ%2F%2FYfu9C0LR3pYDgHpvKan78uL5Tjv7FLn9%2FrOhvScrcUm9YGrFO7LXeRVtUa1yBtHzSGM3KkfbxirFA62amOYwGs0FVc2xtwH7b5M%2BqlfQtTO2eyzq1qFQzWF5ATffhIYNIfKJPY%2FKAGMio4qznrXuJ83ACUSuwiBbgTxx3ZTRlQqRedaGWWZcs%2B4j9LYk8dkyqHU98%3D&X-Amz-Signature=a3364b6f559b6da69bd92a88c04044fd53dafde34d65a09512f9655c5d12e6af&X-Amz-SignedHeaders=host\n",
      "Resolving mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com (mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com)... 2600:1fa0:409f:a4c9:345c:94fa::, 2600:1fa0:4020:d51:34da:d9d9::, 2600:1fa0:40e4:96d0:34da:b961::, ...\n",
      "Connecting to mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com (mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com)|2600:1fa0:409f:a4c9:345c:94fa::|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22487893709 (21G) [application/x-tar]\n",
      "Saving to: ‘en.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ3GQRTO3IFDOHBJP%2F20230325%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230325T174114Z&X-Amz-Expires=43200&X-Amz-Security-Token=FwoGZXIvYXdzEDsaDBC4zPpDJq12Yv6tXSKSBONMEgB’\n",
      "\n",
      "Amz-Algorithm=AWS4-   0%[                    ]  24,91M  6,05MB/s    eta 75m 18s^C\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com/cv-corpus-1/en.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ3GQRTO3IFDOHBJP%2F20230325%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230325T174114Z&X-Amz-Expires=43200&X-Amz-Security-Token=FwoGZXIvYXdzEDsaDBC4zPpDJq12Yv6tXSKSBONMEgB3gdLg5c3lUHnm3%2BucrTCYrctbmwFGg6KjUPbnBzCCboMxHOOcssMQkC5uoDakMhZUm4cOTiw8T5dch%2FS8thbTmObPUr0FiIpmCV65vS%2FRHui6ZIZ5NvaTmjRRk1ZtF1EHDQNeSYouN0xAcpLzjtP2XXkT7kKDPhwP55MdZr87dw4WPk9LbUyNNM%2B7Owqo58vHzBKkEEzs%2FKhkqzBI3CVXkxn8goq%2BeJI1yw8bj4mPAW0DNezP%2FV3a3qPgm%2F20uZ2qodqr3ISMey0HpsVQ4PvFvAIvfasLOGRLBqgnDVIHYZ%2B4pAJ%2FVrDFxqoYXzlINO1XHX6HVGhqmQu%2Bmwwy9J56Hyb%2B%2FLeO3mWEcjjcz7mq55I4CSwZhSkbBP8G8ki5osVn%2BIwGrSyy6d5PYz1CZjs2%2BiBavIPPvRVRvDBpHiC51sMUCUxGj31n5WpG%2B8AhfIpQgsNKq3oAqbVc8u5V7yrwCmIrAqMQ7R2dt%2FgalA7YbwpsHkKVAJjyDgZQV%2FojkNqApfCKA0K%2FiaCn4qOQmWOGTbmWuvgoM9aXJxDwqrzttRtK1zaNnGAuuKwQ%2F%2FYfu9C0LR3pYDgHpvKan78uL5Tjv7FLn9%2FrOhvScrcUm9YGrFO7LXeRVtUa1yBtHzSGM3KkfbxirFA62amOYwGs0FVc2xtwH7b5M%2BqlfQtTO2eyzq1qFQzWF5ATffhIYNIfKJPY%2FKAGMio4qznrXuJ83ACUSuwiBbgTxx3ZTRlQqRedaGWWZcs%2B4j9LYk8dkyqHU98%3D&X-Amz-Signature=a3364b6f559b6da69bd92a88c04044fd53dafde34d65a09512f9655c5d12e6af&X-Amz-SignedHeaders=host' -O dataset.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf foo.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x convert.sh\n",
    "!./convert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([],'GPU')\n",
    "\n",
    "reuse =True\n",
    "maxData=64\n",
    "max_num_words=2000\n",
    "batch_size=32\n",
    "epochs=2\n",
    "latent_dim=512\n",
    "model_name=\"model\"\n",
    "data_folder=\"records_en/\"\n",
    "clips_folder=\"records_en/dataset\"\n",
    "block_lenght=0.5\n",
    "frame_lenght=512\n",
    "voice_max_length=int(8/block_lenght)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioToTensor(filepath:str):\n",
    "    audio_binary=tf.io.read_file(filepath)\n",
    "    audio, audioSR= tf.audio.decode_wav(audio_binary)\n",
    "    audioSR=tf.get_static_value(audioSR)\n",
    "    audio=tf.squeeze(audio, axis=-1)\n",
    "    audio_lenght=int(audioSR*block_lenght)\n",
    "    frame_step=int(audioSR*0.01)\n",
    "\n",
    "    required_lenght=audio_lenght*voice_max_length\n",
    "    if len(audio)<required_lenght:\n",
    "        audio=tf.concat([np.zeros([required_lenght-len(audio)]),audio],0)\n",
    "    else:\n",
    "        audio=audio[-required_lenght]\n",
    "\n",
    "    spectogram = tf.signal.stft(audio, frale_lenght=frame_lenght, frame_step=frame_step)\n",
    "    spectogram = (tf.math.log(tf.abs(tf.math.real(spectogram)))/tf.math.log(tf.constant(10, dtype=tf.float32))*20)-60\n",
    "    spectogram = tf.where(tf.math.is_nan(spectogram),tf.zeros_like(spectogram),spectogram)\n",
    "    spectogram = tf.where(tf.math.is_inf(spectogram), tf.zeros_like(spectogram),spectogram)\n",
    "\n",
    "    return spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleFromFile(filepath):\n",
    "    with open(filepath) as tsvfile:\n",
    "        reader=csv.reader(tsvfile, delimiter='\\t')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            sentence=row[2].replace(\".\",\"\")\n",
    "            wordList=(\"start\"+sentence+\"end\").split(\" \")\n",
    "            if(len(wordList)<5):\n",
    "                continue\n",
    "            return row[1]+\".wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePath = sampleFromFile(os.path.join(data_folder, 'train.tsv'))\n",
    "testParts=audioToTensor(os.path.join(clips_folder, samplePath))\n",
    "print(testParts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadDataFromFile(filepath):\n",
    "    dataVoice, dataString = [], []\n",
    "    string_max_length = 0\n",
    "    with open(filepath) as tsvfile:\n",
    "      reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "      next(reader)#skip header\n",
    "      for row in reader:\n",
    "        if len(dataString)>maxData:\n",
    "            break\n",
    "        sentence = row[2].replace(\".\", \"\")\n",
    "        wordList = (\"start \" + sentence + \" end\").split(\" \")\n",
    "        if(len(wordList)<5):\n",
    "            continue\n",
    "        print(row[1], row[2], wordList)\n",
    "        string_max_length = max(len(wordList), string_max_length)\n",
    "        dataString.append(wordList)\n",
    "        dataVoice.append(row[1]+'.wav')\n",
    "    return dataVoice, dataString, string_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataVoice, dataString, string_max_length = loadDataFromFile(os.path.join(data_folder, 'train.tsv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=tf.keras.preprocessing.text.Tokenizer(num_words=max_num_words,lower=True,oov_token=\"<rare>\")\n",
    "token.fit_on_texts(dataString)\n",
    "with io.open('tokenizer.txt','w',encoding='utf-8') as f:\n",
    "    for word, index in token.word_index.items():\n",
    "        f.write(word + \":\" + str(index)+\"\\n\")\n",
    "vocab_size=min(len(token.word_index)+1,max_num_words)\n",
    "print(\"vocab size :%d\" % vocab_size)\n",
    "\n",
    "def prepareData(dataString, dataVoice):\n",
    "    X_voice, X_string, Y_string =list(), list(), list()\n",
    "    all_seq=token.texts_to_sequences(dataString)\n",
    "    for i, seq in enumerate(all_seq):\n",
    "        voice = dataVoice[i]\n",
    "        for j in range(1,len(seq)):\n",
    "            in_seq, out_seq = seq[:j],[seq[j]]\n",
    "            in_seq=tf.keras.preprocessing.sequence.pad_sequences([in_seq],maxlen=string_max_length)[0]\n",
    "            out_seq=tf.keras.utils.to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "            X_voice.append(voice)\n",
    "            X_string.append(in_seq)\n",
    "            Y_string.append(out_seq)\n",
    "    return X_voice, X_string, Y_string\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_voice, X_string, Y_string = prepareData(dataString, dataVoice)\n",
    "print(\"len(X_voice): \", len(X_voice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_voice,x_string,y_string,batch_size):\n",
    "        self.x_voice, self.x_string, self.y_string, self.batch_size=x_voice, x_string, y_string, batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_voice)//self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_string = self.x_string[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y_string = self.y_string[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x_voice = np.zeros((self.batch_size, testParts.shape[0], testParts.shape[1]))\n",
    "        for i in range(0, batch_size):\n",
    "            batch_x_voice[i] = audioToTensor(os.path.join(clips_folder, self.x_voice[idx * self.batch_size + i]))\n",
    "        batch_x_string = np.array(batch_x_string)\n",
    "        batch_y_string = np.array(batch_y_string)\n",
    "        return [batch_x_voice, batch_x_string], batch_y_string \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(model_name) and reuse:\n",
    "    print(\"Load: \" + model_name)\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "else:\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=(testParts.shape[0], testParts.shape[1]))\n",
    "    encoder_inputs = tf.expand_dims(encoder_inputs, axis=-1)\n",
    "    \n",
    "    preprocessing = tf.keras.layers.experimental.preprocessing.Resizing(400, testParts.shape[1]//2)(encoder_inputs)\n",
    "    normalization = tf.keras.layers.BatchNormalization()(preprocessing)\n",
    "\n",
    "    split = tf.keras.layers.Reshape((voice_max_length, -1, normalization.shape[2], normalization.shape[3]))(normalization)\n",
    "\n",
    "    conv2d = tf.keras.models.TimeDistributed(tf.keras.layers.Conv2D(34, 3, activation='relu'))(split)\n",
    "    conv2d = tf.keras.models.TimeDistributed(tf.keras.layers.Conv2D(64, 3, activation='relu'))(conv2d)\n",
    "    maxpool = tf.keras.models.TimeDistributed(tf.keras.layers.MaxPooling2D())(conv2d)\n",
    "    dropout = tf.keras.models.TimeDistributed(tf.keras.layers.Dropout(0.25))(maxpool)\n",
    "    flatten = tf.keras.models.TimeDistributed(tf.keras.layers.Flatten())(dropout)\n",
    "\n",
    "    encoder_lstm = tf.keras.layers.LSTM(units=latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(flatten)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = keras.Input(shape=(string_max_length))\n",
    "    dec_emb = keras.Embedding(vocab_size, latent_dim)(decoder_inputs)\n",
    "    decoder_outputs = tf.keras.layers.LSTM(units=latent_dim)(dec_emb, initial_state=encoder_states)\n",
    "    decoder_outputs = keras.Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
    "\n",
    "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    tf.keras.utils.plot_model(model, to_file='model_sentence.png', show_shapes=True)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(MySequence(X_voice, X_string, Y_string, batch_size), epochs=epochs, batch_size=batch_size)\n",
    "model.save(model_name)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    decoded_sentence = keras.tokenizer.texts_to_sequences([\"start\"])[0]\n",
    "    while len(decoded_sentence) < string_max_length:\n",
    "        sequence = keras.pad_sequences([decoded_sentence], maxlen=string_max_length)\n",
    "        output_tokens = model.predict([input_seq, sequence], verbose=0)\n",
    "        sampled_token_index = np.argmax(output_tokens[0])\n",
    "        decoded_sentence.append(sampled_token_index)\n",
    "    return keras.tokenizer.sequences_to_texts([decoded_sentence])[0]\n",
    "\n",
    "print(\"Test voice recognition\")\n",
    "for test_path, test_string in [('common_voice_en_346569.wav', \"Do you want me?\"), ('common_voice_en_12677.wav', 'Man in red tshirt and baseball cap viewed from above he is has a pile of posters'), ('common_voice_en_590694.wav', 'Touchscreens do not provide haptic feedback')]:\n",
    "    print(\"test_string: \", test_string)\n",
    "    test_voice = audioToTensor(clips_folder+test_path)\n",
    "    print(np.array([test_voice]).shape)\n",
    "    decoded_sentence = decode_sequence(np.array([test_voice]))\n",
    "    print(\"decoded_sentence: \", decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
